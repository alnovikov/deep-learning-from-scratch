{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a637f17-4fb3-4085-9894-f08dffbb6e27",
   "metadata": {},
   "source": [
    "### Intro\n",
    "\n",
    "This notebook continues to explore gradient calculation and backpropagation, this time experimenting with developing a prototype of an autograd engine. The main goal is to improve an understanding about the process automatic differentiation for computational graphs.\n",
    "\n",
    "A computational graph object will be constructed along with `backprop` method.\n",
    "\n",
    "Plan of attack:\n",
    "\n",
    "`ComputeNode` object which will represent a node in the DAG\n",
    "`Operation` objects representing different operations between the inputs, such as addition, multiplication etc. Those has to be manually defined to compute output value of two inputs, and partial derivative in respect to some input value.\n",
    "`ComputationalGraph` object - a container for the DAG, providing access to the multi-node root and the single-node leaf node. It must have a way to:\n",
    "- propagate forward (`forward_pass`): compute values of each node in respect to its inputs\n",
    "- and backwards (`backprop`): compute gradients of each node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a21d72cc-3708-450a-8fe9-9a6808962f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50721e97-7666-4d86-b268-124daa511b24",
   "metadata": {},
   "source": [
    "### Attempting autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "56a21a84-3daf-45cb-9445-2ec961d3607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputeNode():\n",
    "    def __init__(self, left_input=None, right_input=None, value=None, operation=None):\n",
    "        \"\"\"\n",
    "        operation=None means it is a head node of the graph\n",
    "        \n",
    "        \"\"\"\n",
    "        if left_input is None and right_input is None and value is None:\n",
    "            raise ValueError('Value is N/A and operation is N/A - error in setup of the Node')\n",
    "        self.left_input = left_input\n",
    "        self.right_input = right_input\n",
    "        if left_input is not None and right_input is not None:\n",
    "            self.left_input.next = self\n",
    "            self.right_input.next = self\n",
    "        self.operation = operation\n",
    "        self.value = value\n",
    "        # This will be changed during init\n",
    "        self.next = None\n",
    "        self.gradient = None\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.operation:\n",
    "            return f'ComputeNode: [{self.operation.symbol}]'\n",
    "        else:\n",
    "            return f'ComputeNode: [{self.value}]'\n",
    "\n",
    "    def compute_value(self):\n",
    "        if self.value is not None:\n",
    "            print('Value is alrady calculated for this node')\n",
    "        elif self.operation:\n",
    "            self.value = self.operation.compute(self.left_input, self.right_input)\n",
    "        else:\n",
    "            raise ValueError('Value is N/A and operation is N/A - error in setup of the Node')\n",
    "\n",
    "    def get_value(self):\n",
    "        if self.value is None:\n",
    "            self.compute_value()\n",
    "        return self.value\n",
    "\n",
    "    def compute_gradient(self):\n",
    "        # Gradient of the final node is equal to 1\n",
    "        if self.next is None:\n",
    "            self.gradient = 1\n",
    "        else:\n",
    "            op = self.next.operation\n",
    "            # getting another node\n",
    "            other = self.next.get_other_input(self)\n",
    "            # application of chain rule: df/dx = df/dg * dg/dx\n",
    "            self.gradient = op.partial_derivative(other) * self.next.get_gradient()\n",
    "\n",
    "    def get_gradient(self):\n",
    "        if self.gradient is None:\n",
    "            self.compute_gradient()\n",
    "        return self.gradient\n",
    "\n",
    "    def get_other_input(self, node):\n",
    "        if self.left_input is not None and self.right_input is not None:\n",
    "            if node == self.left_input:\n",
    "                return self.right_input\n",
    "            else:\n",
    "                return self.left_input\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f0001c60-96cc-417b-91c3-ae835e06deae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditionOperation():\n",
    "    symbol = '+'\n",
    "    def compute(left: ComputeNode, right: ComputeNode) -> float:\n",
    "        out = left.get_value() + right.get_value()\n",
    "        return out\n",
    "\n",
    "    def partial_derivative(other_node: ComputeNode):\n",
    "        \"\"\"\n",
    "        other_node - the 'other' input node in the current computational\n",
    "        node, which gradient we want to compute\n",
    "        \"\"\"\n",
    "        return 1\n",
    "\n",
    "class MultiplicationOperation():\n",
    "    symbol = '*'\n",
    "    def compute(left: ComputeNode, right: ComputeNode) -> float:\n",
    "        out = left.get_value() * right.get_value()\n",
    "        return out\n",
    "\n",
    "    def partial_derivative(other_node: ComputeNode):\n",
    "        \"\"\"\n",
    "        other_node - the 'other' input node in the current computational\n",
    "        node, which gradient we want to compute\n",
    "        \"\"\"\n",
    "        return other_node.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f969e60e-8532-4c0f-8040-0085535014f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputationalGraph:\n",
    "    def __init__(self, root: List[ComputeNode], leaf: ComputeNode):\n",
    "        # root can be one or several nodes\n",
    "        self.root = root\n",
    "        self.leaf = leaf\n",
    "\n",
    "\n",
    "    def forward_pass(self):\n",
    "        \"\"\"\n",
    "        Calculates forward pass of the computational graph,\n",
    "        i.e. get value of the function.\n",
    "        \"\"\"\n",
    "        return self.leaf.get_value()\n",
    "\n",
    "    def backprop(self):\n",
    "        \"\"\"\n",
    "        Get gradient vector in respect to the inputs\n",
    "        through backpropagation\n",
    "        \"\"\"\n",
    "        self.forward_pass()\n",
    "        out = [n.get_gradient() for n in self.root]\n",
    "        return out\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83688fe1-5b9c-4c6d-b909-e5e146ab7436",
   "metadata": {},
   "source": [
    "### A simple test\n",
    "\n",
    "Backprop will be performed on the same scalar-valued function `f(x, y, z) = (x + y) * z` at a point [-2, 5, -4].\n",
    "\n",
    "Expected answer: [-4, -4, 3] (see previous notebook for computations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b79b2871-0077-4739-ba05-9199541078a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ComputeNode: [-2]\n",
      "ComputeNode: [5]\n",
      "ComputeNode: [-4]\n",
      "ComputeNode: [+]\n",
      "ComputeNode: [*]\n"
     ]
    }
   ],
   "source": [
    "# graph will be constructed manually\n",
    "x = ComputeNode(value=-2)\n",
    "print(x)\n",
    "y = ComputeNode(value=5)\n",
    "print(y)\n",
    "z = ComputeNode(value=-4)\n",
    "print(z)\n",
    "q = ComputeNode(left_input=x, right_input=y, operation=AdditionOperation)\n",
    "print(q)\n",
    "f = ComputeNode(left_input=q, right_input=z, operation=MultiplicationOperation)\n",
    "print(f)\n",
    "g = ComputationalGraph(root=[x, y, z], leaf=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8c993882-748f-48b6-8c2a-7ab9209305d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df/dx = -4\n",
      "df/dy = -4\n",
      "df/dz = 3\n"
     ]
    }
   ],
   "source": [
    "results = g.backprop()\n",
    "\n",
    "# Print the results\n",
    "print(\"df/dx =\", results[0])\n",
    "print(\"df/dy =\", results[1])\n",
    "print(\"df/dz =\", results[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba93eec0-45bc-42fb-97fa-d932b5beff1a",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "This small proof of concept shows that it is not extremely complicated to create a representation for a computational graph and unwraps computational procedures in backpropagation.\n",
    "\n",
    "Next steps:\n",
    "\n",
    "- Extend number of operations and run this on a real loss function\n",
    "- Move on towards representation of vectorised operations (so far operating only on scalars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

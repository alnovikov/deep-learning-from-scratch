{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7406b14-3f94-4338-84b8-0142aaafcbd1",
   "metadata": {},
   "source": [
    "# Gradient and backpropagation\n",
    "\n",
    "### Intro\n",
    "\n",
    "This notebook looks into functions related to backprop\n",
    "\n",
    "I will use the following function as an example: f = (x + y) * z.\n",
    "\n",
    "We can treat it as a scalar-valued function, taking 3-dimensional vector as an input: [x, y, z].\n",
    "\n",
    "Calculation of vector of partial derivaties (gradient) is a critical part of the training process for neural networks.\n",
    "\n",
    "### Why\n",
    "\n",
    "The main rationale for calculating gradient of the loss function with respect to the model's parameters is to be able to train the neural network, i.e. to be able to estimate a set of parameters minimising loss function. Gradient is used because it mathematically guarantees that it is the best direction to update the weights to decrease value of the loss function. This algorithm is called **gradient descent**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4204c84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sympy import symbols, diff, lambdify\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import os\n",
    "from typing import Callable\n",
    "\n",
    "# Surpress warnings from tensorflow\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a34f36b-5773-44c8-a3a9-e89981bec161",
   "metadata": {},
   "source": [
    "## Computing the gradient\n",
    "\n",
    "There are ways to compute gradient:\n",
    "\n",
    "- *Numerical gradient* (finite differences methods). The numerical gradient is computed by perturbing the parameters of the function slightly and observing the corresponding change in the function's output. The problem is, it is linear in the number of parameters, hence is not efficient.\n",
    "- *Analytical* (by using calculus). It's computation of the gradient of a function with respect to its parameters using symbolic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082b7231-b1ba-4cb1-9336-a2324d630c76",
   "metadata": {},
   "source": [
    "### Numerical gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "161290f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function is taken from the course notes https://cs231n.github.io by Andrej Karpathy\n",
    "\n",
    "def eval_numerical_gradient(f: Callable, x: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    \n",
    "    Computes numerical gradient of f at x\n",
    "    \n",
    "    f - some scalar-valued function\n",
    "    x - np.array, the data point to evaluate gradient at\n",
    "\n",
    "    Logic is the following:\n",
    "    - compute value of the function\n",
    "    - iterate over the input parameters of the function, increment each\n",
    "      by a tiny margin h and note the effect of change in this parameter\n",
    "      on the function value (i.e. partial derivative)\n",
    "    - return the vector of partial derivatives (gradient)\n",
    "    \"\"\"\n",
    "    fx = f(x)\n",
    "    grad = np.zeros(x.shape)\n",
    "    # Some small value to increment function value at \n",
    "    h = 0.000001\n",
    "\n",
    "    # Efficient multi-dimensional iterator object to iterate over arrays\n",
    "    iterator = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "\n",
    "    while not iterator.finished:\n",
    "\n",
    "        # Evaluate function at x + h\n",
    "        ix = iterator.multi_index\n",
    "        old_value = x[ix]\n",
    "        print('old value of parameter:', old_value)\n",
    "\n",
    "        # Increment by h\n",
    "        x[ix] = old_value + h\n",
    "\n",
    "        print('new value of parameter:', old_value + h)\n",
    "        fxh = f(x)\n",
    "        print('new value of the function:', fxh)\n",
    "        # Restore to old value\n",
    "        x[ix] = old_value\n",
    "        # Compute partial derivative\n",
    "        grad[ix] = (fxh - fx) / h\n",
    "        iterator.iternext()\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "04b328dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(params: np.array) -> float: \n",
    "    x, y, z = params\n",
    "    return (x + y) * z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "782409a8-98fe-48dd-a760-4ce7858669c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9e16d4d8-4f5b-447b-b062-0c4facb30cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value of the function at this point\n",
    "x = np.array([-2.0, 5.0, -4.0])\n",
    "f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "20257580-1be1-4bde-8b79-ed0760bb0935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old value of parameter: -2.0\n",
      "new value of parameter: -1.99999\n",
      "new value of the function: -12.00004\n",
      "old value of parameter: 5.0\n",
      "new value of parameter: 5.00001\n",
      "new value of the function: -12.000039999999998\n",
      "old value of parameter: -4.0\n",
      "new value of parameter: -3.99999\n",
      "new value of the function: -11.99997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-4., -4.,  3.])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_numerical_gradient(f, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729a9ba1-d1c7-4bf8-af9b-d35a92f24590",
   "metadata": {},
   "source": [
    "### Analytical gradient & backpropagation\n",
    "\n",
    "Analytical approach to calculation of gradient uses calculus to compute partial derivaties. \n",
    "\n",
    "Backpropagation is an algorithm for efficiently computing gradients in composite functions by applying the chain rule of calculus. It involves propagating the error backward through the \"computational graph\" to calculate the gradients with respect to each parameter of the function.\n",
    "\n",
    "Here is an excellent blog post from Chris Olah on the topic: [Calculus on Computational Graphs: Backpropagation](https://colah.github.io/posts/2015-08-Backprop/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487f0945-51b5-4ed6-85ce-4fd80d64d2a5",
   "metadata": {},
   "source": [
    "#### Backprop by hand: an example from cs231n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1c053d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: 3\n",
      "f: -12\n",
      "[-4, -4, 3]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# f = (x + y) * z\n",
    "# inputs\n",
    "x, y, z = -2, 5, -4\n",
    "\n",
    "# break f into q * z\n",
    "q = x + y\n",
    "print(f'q: {q}')\n",
    "f = q * z\n",
    "print(f'f: {f}')\n",
    "\n",
    "# partial derivatives of z and q in respect to f(x)\n",
    "dfdz = q\n",
    "dfdq = z\n",
    "\n",
    "# partial derivatives of x and y in respect to f(q)\n",
    "dqdx = 1\n",
    "dqdy = 1\n",
    "\n",
    "# Application of chain rule\n",
    "dfdx = dqdx * dfdq\n",
    "dfdy = dqdy * dfdq\n",
    "print([dfdx, dfdy, dfdz])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416fe6c4-5b1e-4b98-9de6-4474cd3a5239",
   "metadata": {},
   "source": [
    "#### Same with symbolic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "802c3cff-dedb-4780-90f2-92ad46a440e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df/dx = -4.0\n",
      "df/dy = -4.0\n",
      "df/dz = 3.0\n"
     ]
    }
   ],
   "source": [
    "x, y, z = symbols('x y z')\n",
    "f = (x + y) * z\n",
    "df_dx = diff(f, x)\n",
    "df_dy = diff(f, y)\n",
    "df_dz = diff(f, z)\n",
    "\n",
    "df_dx_numeric = lambdify((x, y, z), df_dx, 'numpy')\n",
    "df_dy_numeric = lambdify((x, y, z), df_dy, 'numpy')\n",
    "df_dz_numeric = lambdify((x, y, z), df_dz, 'numpy')\n",
    "\n",
    "v = [-2.0, 5.0, -4.0]\n",
    "print(\"df/dx =\", df_dx_numeric(*v))\n",
    "print(\"df/dy =\", df_dy_numeric(*v))\n",
    "print(\"df/dz =\", df_dz_numeric(*v))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89acdf4-6d96-424d-89cd-15659459e30e",
   "metadata": {},
   "source": [
    "#### Same with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9da59aed-799b-496f-a637-33333c235369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df/dx = -4.0\n",
      "df/dy = -4.0\n",
      "df/dz = 3.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = tf.Variable(-2.0)\n",
    "y = tf.Variable(5.0)\n",
    "z = tf.Variable(-4.0)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    f = (x + y) * z\n",
    "    df_dx = tape.gradient(f, x)\n",
    "    df_dy = tape.gradient(f, y)\n",
    "    df_dz = tape.gradient(f, z)\n",
    "\n",
    "# Print the results\n",
    "print(\"df/dx =\", df_dx.numpy())\n",
    "print(\"df/dy =\", df_dy.numpy())\n",
    "print(\"df/dz =\", df_dz.numpy())\n",
    "\n",
    "del tape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee390b91-8dac-47d0-9de8-293fdcd4889b",
   "metadata": {},
   "source": [
    "#### Same with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60cb6a9a-9bf6-4fa4-8460-0c485dba67e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df/dx = -4.0\n",
      "df/dy = -4.0\n",
      "df/dz = 3.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(-2.0, requires_grad=True)\n",
    "y = torch.tensor(5.0, requires_grad=True)\n",
    "z = torch.tensor(-4.0, requires_grad=True)\n",
    "\n",
    "f = (x + y) * z\n",
    "f.backward()\n",
    "\n",
    "# Access the partial derivatives\n",
    "df_dx = x.grad\n",
    "df_dy = y.grad\n",
    "df_dz = z.grad\n",
    "\n",
    "# Print the results\n",
    "print(\"df/dx =\", df_dx.item())\n",
    "print(\"df/dy =\", df_dy.item())\n",
    "print(\"df/dz =\", df_dz.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49b4626-72fa-4449-b9f0-d9054423600c",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The next step is to apply this approach on the vector-valued function to obtain the Jacobian and use this to effectively perform the parameter update."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
